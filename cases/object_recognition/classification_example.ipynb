{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../..')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first step is to prepare the dataset in the pytorch Dataset format.\n",
    "For example like this:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "train_dataset_path = '/home/void/Downloads/land-use_scene_classification/images_train_test_val/train' # your path to train part of dataset\n",
    "val_dataset_path = '/home/void/Downloads/land-use_scene_classification/images_train_test_val/validation' # your path to validation part of dataset\n",
    "\n",
    "transform = Compose([ToTensor(), Resize((256, 256))])\n",
    "train_dataset = ImageFolder(root=train_dataset_path, transform=transform)\n",
    "val_dataset = ImageFolder(root=val_dataset_path, transform=transform)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ImageFolder is a generic data loader where the images are arranged in this way by default:\n",
    "\n",
    "root/dog/xxx.png\n",
    "root/dog/xxy.png\n",
    "root/dog/[...]/xxz.png\n",
    "\n",
    "root/cat/123.png\n",
    "root/cat/nsdf3.png\n",
    "root/cat/[...]/asd932_.png"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second step is model initialization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "from core.architecture.experiment.nn_experimenter import ClassificationExperimenter\n",
    "\n",
    "experimenter = ClassificationExperimenter(\n",
    "    model=resnet18(num_classes=21), # Number of classes in your dataset\n",
    "    name='ResNet18'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18, using device: cuda\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:22<00:00, 10.09it/s, loss=2.12]\n",
      "100%|██████████| 66/66 [00:02<00:00, 24.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1 score: 0.4137087237755503\n",
      "Saved to /home/void/workspace/Fedot.Industrial/cases/object_recognition/models/land-use_scene_classification/ResNet18/train.sd.pt.\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:21<00:00, 10.57it/s, loss=1.5] \n",
      "100%|██████████| 66/66 [00:02<00:00, 22.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:22<00:00, 10.21it/s, loss=1.16]\n",
      "100%|██████████| 66/66 [00:02<00:00, 23.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1 score: 0.5278234668267437\n",
      "Saved to /home/void/workspace/Fedot.Industrial/cases/object_recognition/models/land-use_scene_classification/ResNet18/train.sd.pt.\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:21<00:00, 10.53it/s, loss=0.953]\n",
      "100%|██████████| 66/66 [00:02<00:00, 24.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:21<00:00, 10.56it/s, loss=0.807]\n",
      "100%|██████████| 66/66 [00:02<00:00, 24.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1 score: 0.6721893744975567\n",
      "Saved to /home/void/workspace/Fedot.Industrial/cases/object_recognition/models/land-use_scene_classification/ResNet18/train.sd.pt.\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:21<00:00, 10.49it/s, loss=0.683]\n",
      "100%|██████████| 66/66 [00:02<00:00, 24.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1 score: 0.7066272883809721\n",
      "Saved to /home/void/workspace/Fedot.Industrial/cases/object_recognition/models/land-use_scene_classification/ResNet18/train.sd.pt.\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:20<00:00, 10.96it/s, loss=0.563]\n",
      "100%|██████████| 66/66 [00:02<00:00, 24.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:20<00:00, 10.98it/s, loss=0.457]\n",
      "100%|██████████| 66/66 [00:02<00:00, 23.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1 score: 0.7716378903263468\n",
      "Saved to /home/void/workspace/Fedot.Industrial/cases/object_recognition/models/land-use_scene_classification/ResNet18/train.sd.pt.\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:21<00:00, 10.92it/s, loss=0.391]\n",
      "100%|██████████| 66/66 [00:02<00:00, 24.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1 score: 0.7991579652128119\n",
      "Saved to /home/void/workspace/Fedot.Industrial/cases/object_recognition/models/land-use_scene_classification/ResNet18/train.sd.pt.\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:20<00:00, 10.98it/s, loss=0.329]\n",
      "100%|██████████| 66/66 [00:02<00:00, 24.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1 score: 0.823477053224501\n",
      "Saved to /home/void/workspace/Fedot.Industrial/cases/object_recognition/models/land-use_scene_classification/ResNet18/train.sd.pt.\n"
     ]
    }
   ],
   "source": [
    "experimenter.fit(\n",
    "    dataset_name='land-use_scene_classification', # Name of your dataset\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    num_epochs=10,\n",
    "    dataloader_params={'batch_size': 32, 'num_workers': 8}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you can get predictions for all images in a folder. The method returns a dictionary {'image_name': class}."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experimenter.predict('') # Path to your folder with images"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
